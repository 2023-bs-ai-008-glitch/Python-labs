{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cea1a1-7935-4236-88c9-4aa79bf60f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading missing NLTK data: stopwords...\n",
      "Downloading missing NLTK data: wordnet...\n",
      "Downloading missing NLTK data: averaged_perceptron_tagger...\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, messagebox\n",
    "import re # Keep re for robustness, but prioritize NLTK tokenizer\n",
    "\n",
    "# --- NLTK Imports Re-enabled for requested features (Stemming, Lemmatization, POS) ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# --- Sklearn Imports Re-enabled for Vectorization ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "# --- NLTK Data Downloads (FIXED) ---\n",
    "# Ensure necessary NLTK data is available. We define a helper function \n",
    "# to check and download resources if they are missing.\n",
    "def ensure_nltk_resources():\n",
    "    required_resources = [\n",
    "        \"punkt\", \n",
    "        \"stopwords\", \n",
    "        \"wordnet\", \n",
    "        \"averaged_perceptron_tagger\"\n",
    "    ]\n",
    "    \n",
    "    for resource in required_resources:\n",
    "        try:\n",
    "            # Check if the resource is found locally\n",
    "            nltk.data.find(f'tokenizers/{resource}') # This generic path check works for all\n",
    "        except LookupError:\n",
    "            # If not found, download it\n",
    "            print(f\"Downloading missing NLTK data: {resource}...\")\n",
    "            # Catching the correct exception (nltk.DownloadError) is critical here\n",
    "            try:\n",
    "                nltk.download(resource, quiet=True)\n",
    "            except nltk.DownloadError as e:\n",
    "                # Handle cases where download itself fails (e.g., network issue)\n",
    "                print(f\"Error downloading {resource}: {e}\")\n",
    "                # We show a warning to the user that the resource is needed\n",
    "                messagebox.showwarning(\n",
    "                    \"NLTK Data Missing\", \n",
    "                    f\"Required NLTK resource '{resource}' could not be automatically downloaded. Please ensure you have network access or run 'import nltk; nltk.download(\\\"{resource}\\\")' manually.\"\n",
    "                )\n",
    "\n",
    "# Execute the check/download once when the script starts\n",
    "ensure_nltk_resources()\n",
    "\n",
    "\n",
    "# --- Core NLP Pipeline Logic ---\n",
    "\n",
    "def execute_nlp_pipeline(input_text, steps_config):\n",
    "    \"\"\"\n",
    "    Executes the configured NLP pipeline, focusing on Tokenization, Cleaning,\n",
    "    Stemming, Lemmatization, POS Tagging, BoW, and TF-IDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initial Tokenization\n",
    "    current_tokens = word_tokenize(input_text)\n",
    "    \n",
    "    pipeline_results = {\n",
    "        \"01_Initial Tokens\": current_tokens\n",
    "    }\n",
    "    \n",
    "    # 2. Lowercasing (Part of \"Tokens and Clean\")\n",
    "    if steps_config[\"use_lowercase\"]:\n",
    "        current_tokens = [t.lower() for t in current_tokens]\n",
    "        pipeline_results[\"02_Lowercase Tokens\"] = current_tokens\n",
    "        \n",
    "    # 3. Stopword Removal (Part of \"Tokens and Clean\")\n",
    "    if steps_config[\"filter_stopwords\"]:\n",
    "        english_stops = set(stopwords.words(\"english\"))\n",
    "        # Filter out non-alphabetic tokens and stopwords\n",
    "        current_tokens = [word for word in current_tokens if word.isalpha() and word not in english_stops]\n",
    "        pipeline_results[\"03_Filtered Tokens\"] = current_tokens\n",
    "\n",
    "    # 4. Lemmatization (Lematizzation)\n",
    "    if steps_config[\"use_lemmatization\"]:\n",
    "        # Lemmatization needs POS tags to be effective, but we proceed with default WordNet behavior\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        try:\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in current_tokens]\n",
    "            pipeline_results[\"04_Lemmatized Tokens\"] = lemmatized_tokens\n",
    "            current_tokens = lemmatized_tokens\n",
    "        except LookupError:\n",
    "            # WordNet is missing, show error to prevent crash\n",
    "            messagebox.showerror(\"NLTK Error\", \"WordNet data is missing. Lemmatization failed.\")\n",
    "            pipeline_results[\"04_Lemmatized Tokens\"] = [\"Error: WordNet data missing.\"]\n",
    "            \n",
    "\n",
    "    # 5. Stemming (Stemming)\n",
    "    if steps_config[\"use_stemming\"]:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in current_tokens]\n",
    "        pipeline_results[\"05_Stemmed Tokens\"] = stemmed_tokens\n",
    "        current_tokens = stemmed_tokens\n",
    "        \n",
    "    # 6. Part-of-Speech (POS) Tagging (Pos Tagging)\n",
    "    if steps_config[\"compute_pos\"]:\n",
    "        try:\n",
    "            pos_tags = pos_tag(current_tokens)\n",
    "            pipeline_results[\"06_POS Tags\"] = pos_tags\n",
    "        except LookupError:\n",
    "             messagebox.showerror(\"NLTK Error\", \"POS Tagger data is missing. POS Tagging failed.\")\n",
    "             pipeline_results[\"06_POS Tags\"] = [\"Error: POS Tagger data missing.\"]\n",
    "\n",
    "    # --- Vectorization Steps ---\n",
    "    \n",
    "    # Check if we have any tokens left for vectorization\n",
    "    if not current_tokens:\n",
    "        # Avoid error if all tokens were removed (e.g., text was just \"The a an\")\n",
    "        if steps_config[\"compute_bow\"] or steps_config[\"compute_tfidf\"]:\n",
    "             messagebox.showwarning(\"Empty Tokens\", \"No tokens remain after cleaning; cannot compute vectors.\")\n",
    "        return pipeline_results\n",
    "        \n",
    "    # Prepare text for vectorizers (they expect a list of documents)\n",
    "    processed_doc = \" \".join(current_tokens)\n",
    "\n",
    "    # 7. Bag of Words (BoW)\n",
    "    if steps_config[\"compute_bow\"]:\n",
    "        # Use token_pattern to match tokens we already created\n",
    "        vectorizer_bow = CountVectorizer(token_pattern=r'\\b\\w+\\b') \n",
    "        X_bow = vectorizer_bow.fit_transform([processed_doc])\n",
    "        \n",
    "        pipeline_results[\"07_BoW Vocabulary\"] = vectorizer_bow.get_feature_names_out().tolist()\n",
    "        pipeline_results[\"08_BoW Vector (Counts)\"] = X_bow.toarray().tolist() # Vector/Matrix output\n",
    "\n",
    "    # 8. TF-IDF\n",
    "    if steps_config[\"compute_tfidf\"]:\n",
    "        vectorizer_tfidf = TfidfVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "        X_tfidf = vectorizer_tfidf.fit_transform([processed_doc])\n",
    "        \n",
    "        pipeline_results[\"09_TF-IDF Vocabulary\"] = vectorizer_tfidf.get_feature_names_out().tolist()\n",
    "        # Round scores for cleaner output display\n",
    "        tfidf_scores = [[round(score, 4) for score in row] for row in X_tfidf.toarray()]\n",
    "        pipeline_results[\"10_TF-IDF Vector (Scores)\"] = tfidf_scores # Vector/Matrix output\n",
    "\n",
    "    return pipeline_results\n",
    "\n",
    "\n",
    "# --- Tkinter GUI Implementation ---\n",
    "\n",
    "class NlpApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"Advanced NLP Feature Explorer\")\n",
    "        self.geometry(\"700x750\") \n",
    "        self.configure(bg='#e8f0f8')\n",
    "        \n",
    "        # Define modern styles\n",
    "        style = ttk.Style(self)\n",
    "        style.theme_use('clam')\n",
    "        style.configure('TFrame', background='#e8f0f8')\n",
    "        style.configure('TLabel', background='#e8f0f8', font=('Helvetica', 10))\n",
    "        style.configure('TCheckbutton', background='#e8f0f8', font=('Helvetica', 10))\n",
    "        style.configure('TButton', font=('Helvetica', 10, 'bold'), background='#007bff', foreground='white')\n",
    "        style.map('TButton', background=[('active', '#0056b3')])\n",
    "\n",
    "\n",
    "        # Variables to store checkbox states - NOW INCLUDING BoW and TF-IDF\n",
    "        self.option_vars = {\n",
    "            \"use_lowercase\": tk.BooleanVar(value=True),\n",
    "            \"filter_stopwords\": tk.BooleanVar(value=True),\n",
    "            \"use_stemming\": tk.BooleanVar(value=False), \n",
    "            \"use_lemmatization\": tk.BooleanVar(value=False), \n",
    "            \"compute_pos\": tk.BooleanVar(value=False),\n",
    "            \"compute_bow\": tk.BooleanVar(value=False),       # ADDED\n",
    "            \"compute_tfidf\": tk.BooleanVar(value=False),     # ADDED\n",
    "        }\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Title\n",
    "        title_label = tk.Label(\n",
    "            self,\n",
    "            text=\"Advanced NLP Feature Explorer\",\n",
    "            font=(\"Helvetica\", 18, \"bold\"),\n",
    "            bg=\"#007bff\",\n",
    "            fg=\"white\",\n",
    "            pady=10\n",
    "        )\n",
    "        title_label.pack(fill='x', pady=(10, 5), padx=10)\n",
    "\n",
    "        main_frame = ttk.Frame(self, padding=\"10 10 10 10\")\n",
    "        main_frame.pack(pady=10, padx=10, fill='both', expand=True)\n",
    "\n",
    "        # 1. Input Section\n",
    "        input_frame = ttk.LabelFrame(main_frame, text=\"Input Text\", padding=\"10\")\n",
    "        input_frame.pack(fill='x', pady=5)\n",
    "        \n",
    "        self.text_input = scrolledtext.ScrolledText(\n",
    "            input_frame, \n",
    "            wrap=tk.WORD, \n",
    "            width=80, \n",
    "            height=6, \n",
    "            font=(\"Arial\", 10),\n",
    "            padx=5, pady=5, \n",
    "            bd=1, relief=tk.SUNKEN\n",
    "        )\n",
    "        # Set a default value\n",
    "        self.text_input.insert(tk.END, \"The large computers are rapidly changing the world of data science.\")\n",
    "        self.text_input.pack(fill='x', expand=True)\n",
    "\n",
    "        # 2. Options Section\n",
    "        options_frame = ttk.LabelFrame(main_frame, text=\"Select Linguistic Processing Steps (Requires NLTK & Scikit-learn)\", padding=\"10\")\n",
    "        options_frame.pack(fill='x', pady=10)\n",
    "        \n",
    "        row_idx, col_idx = 0, 0\n",
    "        \n",
    "        # Mapping for display names (updated for all requested options)\n",
    "        display_map = {\n",
    "            \"use_lowercase\": \"Tokens & Clean: Lowercase\",\n",
    "            \"filter_stopwords\": \"Tokens & Clean: Stopword Removal\",\n",
    "            \"use_stemming\": \"Stemming (Porter)\",\n",
    "            \"use_lemmatization\": \"Lemmatization\",\n",
    "            \"compute_pos\": \"POS Tagging (Part-of-Speech)\",\n",
    "            \"compute_bow\": \"BoW (Frequency Vector)\",\n",
    "            \"compute_tfidf\": \"TF-IDF (Score Vector)\",\n",
    "        }\n",
    "\n",
    "        for key, var in self.option_vars.items():\n",
    "            ttk.Checkbutton(\n",
    "                options_frame,\n",
    "                text=display_map.get(key, key),\n",
    "                variable=var,\n",
    "                style='TCheckbutton'\n",
    "            ).grid(row=row_idx, column=col_idx, padx=15, pady=5, sticky=\"w\")\n",
    "            \n",
    "            col_idx += 1\n",
    "            if col_idx > 2:\n",
    "                col_idx = 0\n",
    "                row_idx += 1\n",
    "\n",
    "        # 3. Process Button\n",
    "        process_button = ttk.Button(\n",
    "            main_frame,\n",
    "            text=\"Execute Linguistic Analysis\",\n",
    "            command=self.run_analysis,\n",
    "            style='TButton'\n",
    "        )\n",
    "        process_button.pack(pady=10)\n",
    "\n",
    "        # 4. Output Section\n",
    "        output_frame = ttk.LabelFrame(main_frame, text=\"Analysis Results\", padding=\"10\")\n",
    "        output_frame.pack(fill='both', expand=True, pady=5)\n",
    "\n",
    "        self.output_text = scrolledtext.ScrolledText(\n",
    "            output_frame,\n",
    "            wrap=tk.WORD,\n",
    "            width=80,\n",
    "            height=15,\n",
    "            font=(\"Consolas\", 9),\n",
    "            padx=5, pady=5,\n",
    "            bd=1, relief=tk.SUNKEN\n",
    "        )\n",
    "        self.output_text.pack(fill='both', expand=True)\n",
    "        self.output_text.insert(tk.END, \"Results will appear here after execution.\\n\\nNote: This tool uses NLTK and scikit-learn features.\")\n",
    "        \n",
    "    def run_analysis(self):\n",
    "        \"\"\"Retrieves input, runs the pipeline, and displays output.\"\"\"\n",
    "        \n",
    "        input_text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        \n",
    "        if not input_text:\n",
    "            messagebox.showerror(\"Input Error\", \"Please enter text to analyze.\")\n",
    "            return\n",
    "\n",
    "        steps_config = {key: var.get() for key, var in self.option_vars.items()}\n",
    "        \n",
    "        if not any(steps_config.values()):\n",
    "            messagebox.showwarning(\"Selection Warning\", \"Please select at least one preprocessing step.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            results = execute_nlp_pipeline(input_text, steps_config)\n",
    "            self.display_results(results)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Processing Error\", f\"An error occurred during text processing: {e}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "\n",
    "    def display_results(self, results):\n",
    "        \"\"\"Formats and inserts the results into the output text box.\"\"\"\n",
    "        \n",
    "        self.output_text.delete(\"1.0\", tk.END)\n",
    "        \n",
    "        # Sort results by the numerical prefix for sequential display\n",
    "        sorted_keys = sorted(results.keys())\n",
    "        \n",
    "        for key in sorted_keys:\n",
    "            value = results[key]\n",
    "            # Clean key name for display\n",
    "            clean_key = key.split('_', 1)[-1].replace('_', ' ')\n",
    "            \n",
    "            self.output_text.insert(tk.END, f\"--- {clean_key} ---\\n\", 'header')\n",
    "            \n",
    "            if isinstance(value, list) and all(isinstance(v, list) for v in value):\n",
    "                # Handle Vector/Matrix outputs (BoW and TF-IDF)\n",
    "                self.output_text.insert(tk.END, \"Vector (Document 1):\\n\") \n",
    "                # Format vector rows neatly\n",
    "                for row in value:\n",
    "                    formatted_row = [str(item) for item in row]\n",
    "                    self.output_text.insert(tk.END, f\"  [ {', '.join(formatted_row)} ]\\n\")\n",
    "            elif key == \"06_POS Tags\":\n",
    "                # Special formatting for POS tags\n",
    "                pos_text = ' '.join([f\"({word}, {tag})\" for word, tag in value])\n",
    "                self.output_text.insert(tk.END, f\"  {pos_text}\\n\\n\")\n",
    "            elif isinstance(value, list):\n",
    "                # Handle Tokens/Vocabulary Lists\n",
    "                self.output_text.insert(tk.END, f\"  {', '.join(map(str, value))}\\n\\n\")\n",
    "            else:\n",
    "                self.output_text.insert(tk.END, f\"  {str(value)}\\n\\n\")\n",
    "\n",
    "        # Configure tag for bold header text\n",
    "        self.output_text.tag_config('header', font=('Consolas', 10, 'bold'), foreground='#0056b3')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the application\n",
    "    app = NlpApp()\n",
    "    app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec49ae1-4776-4bee-927a-ab6ae4f4ed38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
